# dsml

A hands-on repository of **Data Science** and **Machine Learning** models built entirely **from scratch** â€” no TensorFlow, no PyTorch â€” just pure Python ğŸ and NumPy ğŸ“Š. Ideal for learners who want to dig into the **core mechanics** of how machine learning actually works.

---

## ğŸ“š Table of Contents

1. [ğŸŒŸ Overview](#-overview)
2. [ğŸ” Features](#-features)
3. [ğŸ§  Mathematical Foundations](#-mathematical-foundations)
4. [âš™ï¸ Implementation Details](#-implementation-details)
5. [ğŸš€ Installation](#-installation)
6. [ğŸ¯ Usage](#-usage)
7. [ğŸ—‚ Sample Data](#-sample-data)
8. [ğŸ“ˆ Results](#-results)
9. [ğŸ›  Extending](#-extending)
10. [ğŸ“ Project Structure](#-project-structure)
11. [ğŸ“œ License](#-license)
12. [âœï¸ Author](#ï¸-author)

---

## ğŸŒŸ Overview

This repo builds a **configurable feedforward neural network** from the ground up. By avoiding high-level libraries, youâ€™ll:

* Understand matrix calculus & vectorized ops ğŸ“
* Manually code each training step ğŸ§ª
* Observe how gradients propagate ğŸ”
* Experiment with layers and activations ğŸ”§

---

## ğŸ” Features

* âœ… Fully-connected multi-layer architecture
* âš¡ Activation functions: Sigmoid, Tanh, ReLU
* ğŸ’¥ Loss functions: MSE, Binary Cross-Entropy
* ğŸŒ€ Full-batch gradient descent optimizer
* ğŸ“Š Visual loss tracking via `matplotlib`
* ğŸ’¡ Clean, modular NumPy code

---

## ğŸ§  Mathematical Foundations

### ğŸ§± Neural Network Layers

Each layer $\ell$ performs:

$$
Z^{[\ell]} = W^{[\ell]} A^{[\ell-1]} + b^{[\ell]} \\
A^{[\ell]} = g(Z^{[\ell]})
$$

Where:

* $W^{[\ell]}$ = weight matrix
* $b^{[\ell]}$ = bias vector
* $g$ = activation function

### â• Forward Propagation

Final prediction:

$$
\hat{Y} = A^{[L]}
$$

### âŒ Loss Functions

**Mean Squared Error (MSE):**

$$
\mathcal{L}_{\text{MSE}} = \frac{1}{m} \sum_{i=1}^m (y^{(i)} - \hat{y}^{(i)})^2
$$

**Binary Cross-Entropy (BCE):**

$$
\mathcal{L}_{\text{BCE}} = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
$$

### ğŸ” Backpropagation

Uses chain rule to compute gradients:

$$
\frac{\partial \mathcal{L}}{\partial W^{[\ell]}} = \delta^{[\ell]} {A^{[\ell-1]}}^T \\
\frac{\partial \mathcal{L}}{\partial b^{[\ell]}} = \sum_{i=1}^{m} \delta^{[\ell](i)}
$$

Error is computed recursively:

$$
\delta^{[\ell]} = (W^{[\ell+1]})^T \delta^{[\ell+1]} \circ g'(Z^{[\ell]})
$$

### ğŸ“‰ Gradient Descent

Parameter update rule:

$$
W^{[\ell]} \leftarrow W^{[\ell]} - \alpha \frac{\partial \mathcal{L}}{\partial W^{[\ell]}} \\
b^{[\ell]} \leftarrow b^{[\ell]} - \alpha \frac{\partial \mathcal{L}}{\partial b^{[\ell]}}
$$

---

## âš™ï¸ Implementation Details

### ğŸ§ª Initialization Methods

* **Random**: $\mathcal{N}(0,1)$
* **Xavier**: $\sqrt{\frac{1}{n_{in}}}$
* **He**: $\sqrt{\frac{2}{n_{in}}}$

### ğŸ”„ Training Loop

```python
for epoch in range(epochs):
    Z[l] = W[l] @ A[l-1] + b[l]
    A[l] = activation(Z[l])
    ...
    dZ = A[L] - Y
    dW = (1/m) * dZ @ A[L-1].T
    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)
    W[l] -= lr * dW
    b[l] -= lr * db
```

---

## ğŸš€ Installation

```bash
git clone https://github.com/humair-m/dsml.git
cd dsml
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**`requirements.txt`**

```
numpy
matplotlib
```

---

## ğŸ¯ Usage

To run the main script:

```bash
python "Neural Network from Scratch.py"
```

To modify configuration:

```python
layer_sizes = [2, 4, 1]
activations = ['tanh', 'sigmoid']
learning_rate = 0.01
epochs = 1000
```

Run with CLI arguments (if using `argparse`):

```bash
python script.py --layers 2 4 1 --activations relu sigmoid --lr 0.01 --epochs 500
```

---

## ğŸ—‚ Sample Data

Uses `make_circles` from `sklearn.datasets` by default.

Optionally, load your own dataset like `img_.pkl`:

```python
import pickle
X, Y = pickle.load(open('img_.pkl', 'rb'))
X = (X - X.mean(axis=1, keepdims=True)) / X.std(axis=1, keepdims=True)
```

---

## ğŸ“ˆ Results

```text
Epoch 1/1000 â€” Loss: 0.6932
Epoch 500/1000 â€” Loss: 0.0513
Epoch 1000/1000 â€” Loss: 0.0189
```

ğŸ–¼ A loss curve is saved as `loss_curve.png`

---

## ğŸ›  Extending

* Add optimizers: Adam, RMSProp, Momentum
* Implement mini-batch or stochastic training
* Use regularization: Dropout, BatchNorm
* Add multiclass support via Softmax + CrossEntropy

---

## ğŸ“ Project Structure

```
dsml/
â”œâ”€â”€ Neural Network from Scratch.py
â”œâ”€â”€ img_.pkl
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ loss_curve.png
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸ“œ License

MIT License â€” see [LICENSE](LICENSE) for full terms.

---

## âœï¸ Author

**Humair M**
GitHub: [@humair-m](https://github.com/humair-m)
ğŸ“… Project started: June 2025
